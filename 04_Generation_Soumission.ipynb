{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Génération de la Soumission - Détection de Fraude\n",
        "\n",
        "## Objectifs de ce notebook\n",
        "1. **Charger le meilleur modèle** identifié lors de l'exploration\n",
        "2. **Préparer les données de test** avec toutes les transformations\n",
        "3. **Générer les prédictions** sur le test set\n",
        "4. **Créer le fichier submission.csv** au format requis\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import des bibliothèques\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bibliothèques standards\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Gestion du déséquilibre\n",
        "try:\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "    IMBLEARN_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"imbalanced-learn non disponible\")\n",
        "    IMBLEARN_AVAILABLE = False\n",
        "\n",
        "# XGBoost (optionnel)\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    XGBOOST_AVAILABLE = True\n",
        "except ImportError:\n",
        "    XGBOOST_AVAILABLE = False\n",
        "\n",
        "print(\"Bibliothèques importées avec succès\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note :** Ce bloc importe toutes les bibliothèques nécessaires pour générer la soumission finale. Il inclut les modèles de machine learning, les outils de preprocessing, et gère les cas où certaines bibliothèques ne sont pas disponibles.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Fonctions de préparation des données\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fonction pour créer les features avancées (identique à l'étape 3)\n",
        "def create_advanced_features(df, is_train=True):\n",
        "    \"\"\"Crée des features avancées pour améliorer les performances\"\"\"\n",
        "    df_adv = df.copy()\n",
        "    \n",
        "    # Features temporelles avancées\n",
        "    df_adv['period_of_day'] = pd.cut(\n",
        "        df_adv['hour_of_day'],\n",
        "        bins=[0, 6, 12, 18, 24],\n",
        "        labels=['Nuit', 'Matin', 'Apres-midi', 'Soir'],\n",
        "        include_lowest=True\n",
        "    )\n",
        "    \n",
        "    # Features d'interaction\n",
        "    df_adv['amount_per_type'] = df_adv.groupby('type')['amount'].transform('mean')\n",
        "    df_adv['amount_ratio'] = df_adv['amount'] / (df_adv['amount_per_type'] + 1)\n",
        "    df_adv['amount_age_ratio'] = df_adv['amount'] / (df_adv['age'] + 1)\n",
        "    \n",
        "    # Features statistiques par groupe\n",
        "    df_adv['avg_amount_by_day'] = df_adv.groupby('day_of_week')['amount'].transform('mean')\n",
        "    df_adv['amount_vs_day_avg'] = df_adv['amount'] / (df_adv['avg_amount_by_day'] + 1)\n",
        "    df_adv['avg_amount_by_hour'] = df_adv.groupby('hour_of_day')['amount'].transform('mean')\n",
        "    df_adv['amount_vs_hour_avg'] = df_adv['amount'] / (df_adv['avg_amount_by_hour'] + 1)\n",
        "    \n",
        "    # Features de seuil\n",
        "    amount_threshold = df_adv['amount'].quantile(0.95)\n",
        "    df_adv['is_high_amount'] = (df_adv['amount'] > amount_threshold).astype(int)\n",
        "    df_adv['is_young'] = (df_adv['age'] < 25).astype(int)\n",
        "    df_adv['is_senior'] = (df_adv['age'] > 65).astype(int)\n",
        "    \n",
        "    # Features cycliques\n",
        "    df_adv['day_sin'] = np.sin(2 * np.pi * df_adv['day_of_week'] / 7)\n",
        "    df_adv['day_cos'] = np.cos(2 * np.pi * df_adv['day_of_week'] / 7)\n",
        "    df_adv['hour_sin'] = np.sin(2 * np.pi * df_adv['hour_of_day'] / 24)\n",
        "    df_adv['hour_cos'] = np.cos(2 * np.pi * df_adv['hour_of_day'] / 24)\n",
        "    \n",
        "    # Features polynomiales\n",
        "    df_adv['amount_squared'] = df_adv['amount'] ** 2\n",
        "    df_adv['age_squared'] = df_adv['age'] ** 2\n",
        "    \n",
        "    return df_adv\n",
        "\n",
        "# Fonction pour encoder les features\n",
        "def encode_features(df, is_train=True):\n",
        "    \"\"\"Encode les features catégorielles\"\"\"\n",
        "    df_enc = df.copy()\n",
        "    \n",
        "    # One-Hot Encoding pour 'type'\n",
        "    type_dummies = pd.get_dummies(df_enc['type'], prefix='type')\n",
        "    all_types = ['type_CASH_OUT', 'type_DEBIT', 'type_PAYMENT', 'type_TRANSFER']\n",
        "    for col in all_types:\n",
        "        if col not in type_dummies.columns:\n",
        "            type_dummies[col] = 0\n",
        "    type_dummies = type_dummies[sorted(all_types)]\n",
        "    \n",
        "    # One-Hot Encoding pour 'period_of_day'\n",
        "    period_dummies = pd.get_dummies(df_enc['period_of_day'], prefix='period')\n",
        "    \n",
        "    # Concaténation\n",
        "    df_enc = pd.concat([df_enc, type_dummies, period_dummies], axis=1)\n",
        "    \n",
        "    return df_enc\n",
        "\n",
        "print(\"Fonctions de preparation des donnees definies\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note :** Ce bloc définit les fonctions de préparation des données qui sont identiques à celles utilisées lors de l'entraînement. Il est crucial d'utiliser exactement les mêmes transformations pour garantir que les données de test sont traitées de la même manière que les données d'entraînement.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Chargement et préparation des données\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chargement des données\n",
        "print(\"=\" * 60)\n",
        "print(\"CHARGEMENT DES DONNEES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "train_df = pd.read_csv('ressources/train_prepared.csv')\n",
        "test_df = pd.read_csv('ressources/test_prepared.csv')\n",
        "\n",
        "print(f\"\\nTrain set: {train_df.shape}\")\n",
        "print(f\"Test set: {test_df.shape}\")\n",
        "\n",
        "# Vérification\n",
        "print(f\"\\nColonnes test: {list(test_df.columns)}\")\n",
        "if 'is_fraud' in test_df.columns:\n",
        "    print(\"ATTENTION: is_fraud present dans le test set (ne devrait pas etre la)\")\n",
        "else:\n",
        "    print(\"OK: is_fraud absent du test set\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Application des transformations\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"PREPARATION DES DONNEES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Features avancées\n",
        "train_adv = create_advanced_features(train_df, is_train=True)\n",
        "test_adv = create_advanced_features(test_df, is_train=False)\n",
        "\n",
        "# Encodage\n",
        "train_enc = encode_features(train_adv, is_train=True)\n",
        "test_enc = encode_features(test_adv, is_train=False)\n",
        "\n",
        "print(f\"\\nTrain avec features: {train_enc.shape}\")\n",
        "print(f\"Test avec features: {test_enc.shape}\")\n",
        "\n",
        "# Sélection des features (identique à l'étape 3)\n",
        "features_to_exclude = [\n",
        "    'transaction_id',\n",
        "    'customer_id',\n",
        "    'type',\n",
        "    'age_group',\n",
        "    'period_of_day',\n",
        "    'is_fraud'\n",
        "]\n",
        "\n",
        "feature_columns = [col for col in train_enc.columns \n",
        "                  if col not in features_to_exclude]\n",
        "\n",
        "# Préparation des datasets\n",
        "X = train_enc[feature_columns].copy()\n",
        "y = train_enc['is_fraud'].copy()\n",
        "X_test = test_enc[feature_columns].copy()\n",
        "\n",
        "print(f\"\\nFeatures selectionnees: {len(feature_columns)}\")\n",
        "print(f\"X shape: {X.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "\n",
        "# Vérification de la cohérence\n",
        "if list(X.columns) == list(X_test.columns):\n",
        "    print(\"\\nLes colonnes sont identiques entre train et test\")\n",
        "else:\n",
        "    print(\"\\nATTENTION: Differences dans les colonnes!\")\n",
        "    missing_in_test = set(X.columns) - set(X_test.columns)\n",
        "    missing_in_train = set(X_test.columns) - set(X.columns)\n",
        "    if missing_in_test:\n",
        "        print(f\"Colonnes manquantes dans test: {missing_in_test}\")\n",
        "    if missing_in_train:\n",
        "        print(f\"Colonnes en trop dans test: {missing_in_train}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note :** Ce bloc charge les données d'entraînement et de test, puis applique toutes les transformations nécessaires (features avancées, encodage) de manière identique à l'entraînement. Il vérifie que les colonnes sont cohérentes entre train et test pour éviter les erreurs lors de la prédiction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Entraînement du meilleur modèle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Division train/validation\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"ENTRAINEMENT DU MEILLEUR MODELE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Application de SMOTE si disponible\n",
        "if IMBLEARN_AVAILABLE:\n",
        "    smote = SMOTE(random_state=42, k_neighbors=5)\n",
        "    X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
        "    print(f\"\\nSMOTE applique - Train shape: {X_train_smote.shape}\")\n",
        "else:\n",
        "    X_train_smote = X_train\n",
        "    y_train_smote = y_train\n",
        "    print(\"\\nSMOTE non disponible - utilisation des donnees originales\")\n",
        "\n",
        "# Entraînement du meilleur modèle (Random Forest avec paramètres optimisés)\n",
        "# Vous pouvez ajuster ces paramètres selon vos résultats de l'étape 3\n",
        "print(\"\\nEntraînement du Random Forest...\")\n",
        "final_model = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=15,\n",
        "    min_samples_split=20,\n",
        "    min_samples_leaf=10,\n",
        "    random_state=42,\n",
        "    class_weight='balanced',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "final_model.fit(X_train_smote, y_train_smote)\n",
        "print(\"Entraînement terminé!\")\n",
        "\n",
        "# Évaluation rapide sur la validation\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "y_val_pred = final_model.predict(X_val)\n",
        "val_f1 = f1_score(y_val, y_val_pred)\n",
        "val_precision = precision_score(y_val, y_val_pred)\n",
        "val_recall = recall_score(y_val, y_val_pred)\n",
        "\n",
        "print(f\"\\nPerformance sur validation:\")\n",
        "print(f\"   - F1-Score: {val_f1:.4f}\")\n",
        "print(f\"   - Precision: {val_precision:.4f}\")\n",
        "print(f\"   - Recall: {val_recall:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note :** Ce bloc entraîne le meilleur modèle identifié lors de l'exploration (ici Random Forest avec paramètres optimisés). Il applique SMOTE si disponible pour gérer le déséquilibre, puis évalue rapidement le modèle sur la validation pour vérifier les performances avant de générer les prédictions finales.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Génération des prédictions et création de submission.csv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prédictions sur le test set\n",
        "print(\"=\" * 60)\n",
        "print(\"GENERATION DES PREDICTIONS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Vérification finale de la cohérence des colonnes\n",
        "if list(X_train.columns) == list(X_test.columns):\n",
        "    # Génération des prédictions\n",
        "    test_predictions = final_model.predict(X_test)\n",
        "    test_proba = final_model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    print(f\"\\nNombre de predictions: {len(test_predictions)}\")\n",
        "    print(f\"Nombre de fraudes predites: {test_predictions.sum()}\")\n",
        "    print(f\"Taux de fraude predit: {test_predictions.mean()*100:.2f}%\")\n",
        "    print(f\"Distribution des probabilites:\")\n",
        "    print(f\"   - Min: {test_proba.min():.4f}\")\n",
        "    print(f\"   - Max: {test_proba.max():.4f}\")\n",
        "    print(f\"   - Moyenne: {test_proba.mean():.4f}\")\n",
        "    \n",
        "    # Création du DataFrame de soumission\n",
        "    submission = pd.DataFrame({\n",
        "        'transaction_id': test_df['transaction_id'],\n",
        "        'is_fraud': test_predictions\n",
        "    })\n",
        "    \n",
        "    # Vérification du format\n",
        "    print(f\"\\nFormat de submission:\")\n",
        "    print(f\"   - Shape: {submission.shape}\")\n",
        "    print(f\"   - Colonnes: {list(submission.columns)}\")\n",
        "    print(f\"   - Types: {submission.dtypes.to_dict()}\")\n",
        "    \n",
        "    # Vérification des valeurs\n",
        "    print(f\"\\nValeurs dans is_fraud:\")\n",
        "    print(submission['is_fraud'].value_counts().sort_index())\n",
        "    \n",
        "    # Sauvegarde\n",
        "    submission.to_csv('submission.csv', index=False)\n",
        "    print(f\"\\nFichier submission.csv cree avec succes!\")\n",
        "    print(f\"   - Emplacement: submission.csv\")\n",
        "    print(f\"   - Nombre de lignes: {len(submission)}\")\n",
        "    \n",
        "    # Aperçu\n",
        "    print(f\"\\nApercu des 10 premieres lignes:\")\n",
        "    display(submission.head(10))\n",
        "    \n",
        "    # Aperçu des fraudes prédites\n",
        "    fraud_predictions = submission[submission['is_fraud'] == 1]\n",
        "    if len(fraud_predictions) > 0:\n",
        "        print(f\"\\nApercu des fraudes predites (10 premieres):\")\n",
        "        display(fraud_predictions.head(10))\n",
        "    \n",
        "else:\n",
        "    print(\"ERREUR: Les colonnes ne correspondent pas!\")\n",
        "    print(f\"Train columns ({len(X_train.columns)}): {list(X_train.columns)[:5]}...\")\n",
        "    print(f\"Test columns ({len(X_test.columns)}): {list(X_test.columns)[:5]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note :** Ce bloc génère les prédictions finales sur le test set et crée le fichier submission.csv au format requis. Il vérifie que les colonnes sont cohérentes, affiche des statistiques sur les prédictions, et sauvegarde le fichier. Le format est exactement celui demandé : transaction_id et is_fraud (0 ou 1).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Vérification finale du fichier de soumission\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vérification finale\n",
        "print(\"=\" * 60)\n",
        "print(\"VERIFICATION FINALE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Recharger le fichier pour vérifier\n",
        "submission_check = pd.read_csv('submission.csv')\n",
        "\n",
        "print(f\"\\nFichier submission.csv:\")\n",
        "print(f\"   - Shape: {submission_check.shape}\")\n",
        "print(f\"   - Colonnes: {list(submission_check.columns)}\")\n",
        "\n",
        "# Vérifications\n",
        "checks = {\n",
        "    \"Nombre de lignes correct\": len(submission_check) == len(test_df),\n",
        "    \"Colonne transaction_id presente\": 'transaction_id' in submission_check.columns,\n",
        "    \"Colonne is_fraud presente\": 'is_fraud' in submission_check.columns,\n",
        "    \"Pas de valeurs manquantes\": submission_check.isnull().sum().sum() == 0,\n",
        "    \"Valeurs is_fraud valides (0 ou 1)\": submission_check['is_fraud'].isin([0, 1]).all(),\n",
        "    \"Pas de doublons transaction_id\": submission_check['transaction_id'].nunique() == len(submission_check)\n",
        "}\n",
        "\n",
        "print(f\"\\nVerifications:\")\n",
        "all_ok = True\n",
        "for check_name, check_result in checks.items():\n",
        "    status = \"OK\" if check_result else \"ERREUR\"\n",
        "    print(f\"   - {check_name}: {status}\")\n",
        "    if not check_result:\n",
        "        all_ok = False\n",
        "\n",
        "if all_ok:\n",
        "    print(f\"\\nToutes les verifications sont OK!\")\n",
        "    print(f\"Le fichier submission.csv est pret pour la soumission.\")\n",
        "else:\n",
        "    print(f\"\\nATTENTION: Certaines verifications ont echoue!\")\n",
        "    print(f\"Veuillez corriger les problemes avant de soumettre.\")\n",
        "\n",
        "# Statistiques finales\n",
        "print(f\"\\nStatistiques finales:\")\n",
        "print(f\"   - Total de transactions: {len(submission_check)}\")\n",
        "print(f\"   - Transactions legitimes (0): {(submission_check['is_fraud'] == 0).sum()}\")\n",
        "print(f\"   - Transactions frauduleuses (1): {(submission_check['is_fraud'] == 1).sum()}\")\n",
        "print(f\"   - Taux de fraude predit: {(submission_check['is_fraud'] == 1).mean()*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note :** Ce bloc effectue une vérification finale du fichier submission.csv pour s'assurer qu'il respecte tous les critères requis : bon nombre de lignes, colonnes correctes, pas de valeurs manquantes, valeurs valides (0 ou 1), et pas de doublons. Ces vérifications sont importantes avant de soumettre le fichier.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Résumé\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"RESUME - GENERATION DE LA SOUMISSION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "summary = {\n",
        "    \"Fichier genere\": {\n",
        "        \"Nom\": \"submission.csv\",\n",
        "        \"Nombre de lignes\": len(submission_check),\n",
        "        \"Colonnes\": list(submission_check.columns)\n",
        "    },\n",
        "    \"Predictions\": {\n",
        "        \"Transactions legitimes (0)\": int((submission_check['is_fraud'] == 0).sum()),\n",
        "        \"Transactions frauduleuses (1)\": int((submission_check['is_fraud'] == 1).sum()),\n",
        "        \"Taux de fraude predit\": f\"{(submission_check['is_fraud'] == 1).mean()*100:.2f}%\"\n",
        "    },\n",
        "    \"Modele utilise\": {\n",
        "        \"Type\": \"Random Forest\",\n",
        "        \"F1-Score (validation)\": f\"{val_f1:.4f}\",\n",
        "        \"Precision (validation)\": f\"{val_precision:.4f}\",\n",
        "        \"Recall (validation)\": f\"{val_recall:.4f}\"\n",
        "    },\n",
        "    \"Prochaines etapes\": {\n",
        "        \"1\": \"Verifier que submission.csv est bien cree\",\n",
        "        \"2\": \"Soumettre le fichier selon les instructions\",\n",
        "        \"3\": \"Attendre les resultats de l'evaluation\"\n",
        "    }\n",
        "}\n",
        "\n",
        "for key, value in summary.items():\n",
        "    print(f\"\\n{key}:\")\n",
        "    if isinstance(value, dict):\n",
        "        for k, v in value.items():\n",
        "            print(f\"   - {k}: {v}\")\n",
        "    else:\n",
        "        print(f\"   {value}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"GENERATION DE LA SOUMISSION TERMINEE\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nLe fichier submission.csv est pret pour la soumission!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note :** Ce bloc final résume toutes les informations importantes sur la génération de la soumission : le fichier créé, les statistiques des prédictions, les performances du modèle utilisé, et les prochaines étapes. Le fichier submission.csv est maintenant prêt à être soumis.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
